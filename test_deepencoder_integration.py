#!/usr/bin/env python3
"""
DeepEncoder é›†æˆæµ‹è¯•è„šæœ¬
========================

æµ‹è¯• DeepEncoder æ˜¯å¦æ­£ç¡®é›†æˆåˆ° GEM/LLaVA æ¶æ„ä¸­

è¿è¡Œæ–¹å¼ï¼š
    python test_deepencoder_integration.py
"""

import torch
import sys
import os
from PIL import Image

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def test_image_processor():
    """æµ‹è¯•å›¾åƒå¤„ç†å™¨"""
    print("\n" + "="*60)
    print("Test 1: Image Processor")
    print("="*60)
    
    from llava.model.multimodal_encoder.deepencoder_tower import DeepEncoderImageProcessor
    
    processor = DeepEncoderImageProcessor()
    
    # æµ‹è¯• Base Mode
    print("\n[Base Mode]")
    test_img = Image.new('RGB', (800, 600), color='blue')
    result = processor(test_img, mode="base")
    print(f"Input size: {test_img.size}")
    print(f"Output shape: {result['pixel_values'].shape}")
    print(f"Expected: [1, 3, 1024, 1024] âœ“" if result['pixel_values'].shape == (1, 3, 1024, 1024) else "âœ—")
    
    # æµ‹è¯• Gundam Mode
    print("\n[Gundam Mode]")
    test_img_large = Image.new('RGB', (1920, 1080), color='red')
    result_gundam = processor([test_img_large], mode="gundam")
    print(f"Input size: {test_img_large.size}")
    print(f"Global view: {result_gundam[0]['global_view'].shape}")
    print(f"Patches: {result_gundam[0]['patches'].shape}")
    print(f"Crop ratio: {result_gundam[0]['crop_ratio']}")
    
    return True


def test_vision_tower():
    """æµ‹è¯• Vision Tower"""
    print("\n" + "="*60)
    print("Test 2: Vision Tower (Base Mode)")
    print("="*60)
    
    from llava.model.multimodal_encoder.deepencoder_tower import DeepEncoderVisionTower
    
    class Args:
        unfreeze_mm_vision_tower = False
        use_gundam_mode = False
    
    args = Args()
    
    # æ³¨æ„ï¼šè¿™é‡Œä¸åŠ è½½çœŸå®æƒé‡ï¼Œåªæµ‹è¯•ç»“æ„
    print("\n[Creating model...]")
    model = DeepEncoderVisionTower(
        vision_tower='/path/to/weights',  # dummy path
        args=args,
        delay_load=False
    )
    
    print(f"Model created successfully!")
    print(f"Hidden size: {model.hidden_size}")
    print(f"Num patches: {model.num_patches}")
    print(f"Image size: {model.config.image_size}")
    
    # æµ‹è¯• forward
    print("\n[Testing forward pass...]")
    dummy_input = torch.randn(2, 3, 1024, 1024)
    
    try:
        with torch.no_grad():
            output = model(dummy_input)
        print(f"Input shape: {dummy_input.shape}")
        print(f"Output shape: {output.shape}")
        print(f"Expected: [2, 272, 2048] (16x16 grid + 16 newlines)")
        
        if output.shape == (2, 272, 2048):
            print("âœ“ Output shape correct!")
            return True
        else:
            print(f"âœ— Output shape mismatch! Got {output.shape}")
            return False
    except Exception as e:
        print(f"âœ— Forward pass failed: {e}")
        return False


def test_collator():
    """æµ‹è¯• Data Collator"""
    print("\n" + "="*60)
    print("Test 3: Data Collator")
    print("="*60)
    
    from llava.model.multimodal_encoder.deepencoder_collator import get_deepencoder_collator
    
    class MockTokenizer:
        pad_token_id = 0
    
    tokenizer = MockTokenizer()
    
    # æ¨¡æ‹Ÿä¸åŒåˆ‡åˆ†æ•°é‡çš„æ ·æœ¬
    instances = [
        {
            'input_ids': torch.tensor([1, 2, 3, 4]),
            'labels': torch.tensor([1, 2, 3, 4]),
            'images': {
                'global_view': torch.randn(3, 1024, 1024),
                'patches': torch.randn(6, 3, 640, 640),
                'crop_ratio': (2, 3)
            },
            'ecgs': torch.randn(12, 5000)
        },
        {
            'input_ids': torch.tensor([1, 2, 3]),
            'labels': torch.tensor([1, 2, 3]),
            'images': {
                'global_view': torch.randn(3, 1024, 1024),
                'patches': torch.randn(9, 3, 640, 640),  # ä¸åŒæ•°é‡!
                'crop_ratio': (3, 3)
            },
            'ecgs': torch.randn(12, 5000)
        }
    ]
    
    # æµ‹è¯• Gundam Collator (List mode)
    print("\n[Gundam Collator - List Mode]")
    collator_gundam = get_deepencoder_collator("gundam", tokenizer)
    batch = collator_gundam(instances)
    
    print(f"Input IDs: {batch['input_ids'].shape}")
    print(f"Images: {len(batch['images'])} samples")
    print(f"  Sample 0 patches: {batch['images'][0]['patches'].shape[0]} patches")
    print(f"  Sample 1 patches: {batch['images'][1]['patches'].shape[0]} patches")
    print(f"ECGs: {batch['ecgs'].shape}")
    print("âœ“ Gundam collator works with variable patch numbers!")
    
    # æµ‹è¯• Padded Collator
    print("\n[Gundam Collator - Padded Mode]")
    collator_padded = get_deepencoder_collator("gundam_padded", tokenizer)
    batch_padded = collator_padded(instances)
    
    print(f"Global Views: {batch_padded['global_views'].shape}")
    print(f"Patches (padded): {batch_padded['patches'].shape}")
    print(f"Patches Mask: {batch_padded['patches_mask'].shape}")
    print("âœ“ Padded collator successfully pads variable-length patches!")
    
    return True


def test_builder_integration():
    """æµ‹è¯• builder.py ä¸­çš„é›†æˆ"""
    print("\n" + "="*60)
    print("Test 4: Builder Integration")
    print("="*60)
    
    from llava.model.multimodal_encoder.builder import build_vision_tower
    
    class Config:
        mm_vision_tower = '/dummy/path'
        vision_tower = '/dummy/path'
        use_deepencoder = True
        use_gundam_mode = False
        unfreeze_mm_vision_tower = False
        delay_load = True
    
    config = Config()
    
    print("\n[Building vision tower with DeepEncoder...]")
    try:
        tower = build_vision_tower(config, delay_load=True)
        print(f"âœ“ Tower type: {type(tower).__name__}")
        print(f"âœ“ Hidden size: {tower.hidden_size}")
        return True
    except Exception as e:
        print(f"âœ— Failed to build tower: {e}")
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*80)
    print(" DeepEncoder Integration Test Suite")
    print("="*80)
    
    results = []
    
    # æµ‹è¯• 1: å›¾åƒå¤„ç†å™¨
    try:
        results.append(("Image Processor", test_image_processor()))
    except Exception as e:
        print(f"\nâœ— Image Processor test failed: {e}")
        results.append(("Image Processor", False))
    
    # æµ‹è¯• 2: Vision Tower
    try:
        results.append(("Vision Tower", test_vision_tower()))
    except Exception as e:
        print(f"\nâœ— Vision Tower test failed: {e}")
        results.append(("Vision Tower", False))
    
    # æµ‹è¯• 3: Collator
    try:
        results.append(("Data Collator", test_collator()))
    except Exception as e:
        print(f"\nâœ— Data Collator test failed: {e}")
        results.append(("Data Collator", False))
    
    # æµ‹è¯• 4: Builder é›†æˆ
    try:
        results.append(("Builder Integration", test_builder_integration()))
    except Exception as e:
        print(f"\nâœ— Builder Integration test failed: {e}")
        results.append(("Builder Integration", False))
    
    # æ€»ç»“
    print("\n" + "="*80)
    print(" Test Summary")
    print("="*80)
    
    for name, passed in results:
        status = "âœ“ PASS" if passed else "âœ— FAIL"
        print(f"{status:10} {name}")
    
    all_passed = all(passed for _, passed in results)
    
    print("\n" + "="*80)
    if all_passed:
        print("ğŸ‰ All tests passed! DeepEncoder is successfully integrated.")
    else:
        print("âš ï¸  Some tests failed. Please check the errors above.")
    print("="*80 + "\n")
    
    return 0 if all_passed else 1


if __name__ == '__main__':
    sys.exit(main())
