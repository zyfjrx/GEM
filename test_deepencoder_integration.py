#!/usr/bin/env python3
"""
æµ‹è¯•DeepEncoderé›†æˆæ˜¯å¦æ­£å¸¸å·¥ä½œ
Usage: python test_deepencoder_integration.py --deepseek_ocr_path /path/to/deepseek-ocr-model
       python test_deepencoder_integration.py --compare_structure  # å¯¹æ¯”æ¨¡åž‹ç»“æž„
"""

import argparse
import torch
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))


def compare_model_structure():
    """å¯¹æ¯”åŽŸå§‹DeepEncoderå’Œé›†æˆåŽçš„æ¨¡åž‹ç»“æž„"""
    print("\n" + "=" * 80)
    print("å¯¹æ¯”æ¨¡åž‹ç»“æž„: åŽŸå§‹ DeepEncoder vs é›†æˆåŽçš„ DeepEncoderVisionTower")
    print("=" * 80)
    
    # 1. åŠ è½½åŽŸå§‹ DeepEncoder ç»„ä»¶
    print("\n" + "-" * 40)
    print("ã€åŽŸå§‹ DeepEncoder ç»„ä»¶ã€‘")
    print("-" * 40)
    
    from deepseek_ocr.deepencoder import build_sam_vit_b, build_clip_l
    
    original_sam = build_sam_vit_b(checkpoint=None)
    original_clip = build_clip_l()
    
    print("\nðŸ“¦ SAM ViT-B (ImageEncoderViT):")
    print(f"   ç±»åž‹: {type(original_sam).__name__}")
    sam_params = sum(p.numel() for p in original_sam.parameters())
    print(f"   å‚æ•°é‡: {sam_params:,} ({sam_params/1e6:.2f}M)")
    
    print("\nðŸ“¦ CLIP-L (VitModel):")
    print(f"   ç±»åž‹: {type(original_clip).__name__}")
    clip_params = sum(p.numel() for p in original_clip.parameters())
    print(f"   å‚æ•°é‡: {clip_params:,} ({clip_params/1e6:.2f}M)")
    
    print(f"\n   æ€»å‚æ•°é‡: {(sam_params + clip_params):,} ({(sam_params + clip_params)/1e6:.2f}M)")
    
    # 2. åŠ è½½é›†æˆåŽçš„ DeepEncoderVisionTower (ç›´æŽ¥å¯¼å…¥æ¨¡å—é¿å…å¤æ‚ä¾èµ–)
    print("\n" + "-" * 40)
    print("ã€é›†æˆåŽçš„ DeepEncoderVisionTowerã€‘")
    print("-" * 40)
    
    # ç›´æŽ¥å¯¼å…¥deepencoder_toweræ¨¡å—ä»¥é¿å…llavaåŒ…çš„å¤æ‚ä¾èµ–
    import importlib.util
    spec = importlib.util.spec_from_file_location(
        "deepencoder_tower", 
        os.path.join(os.path.dirname(__file__), "llava/model/multimodal_encoder/deepencoder_tower.py")
    )
    deepencoder_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(deepencoder_module)
    DeepEncoderVisionTower = deepencoder_module.DeepEncoderVisionTower
    
    class Args:
        mm_vision_select_layer = -2
        mm_vision_select_feature = 'patch'
        unfreeze_mm_vision_tower = False
    
    tower = DeepEncoderVisionTower(vision_tower=None, args=Args(), delay_load=True)
    tower.load_model()
    
    tower_sam_params = sum(p.numel() for p in tower.sam_model.parameters())
    tower_clip_params = sum(p.numel() for p in tower.vision_model.parameters())
    
    print(f"\nðŸ“¦ tower.sam_model:")
    print(f"   ç±»åž‹: {type(tower.sam_model).__name__}")
    print(f"   å‚æ•°é‡: {tower_sam_params:,} ({tower_sam_params/1e6:.2f}M)")
    
    print(f"\nðŸ“¦ tower.vision_model (CLIP-L):")
    print(f"   ç±»åž‹: {type(tower.vision_model).__name__}")
    print(f"   å‚æ•°é‡: {tower_clip_params:,} ({tower_clip_params/1e6:.2f}M)")
    
    print(f"\n   æ€»å‚æ•°é‡: {(tower_sam_params + tower_clip_params):,} ({(tower_sam_params + tower_clip_params)/1e6:.2f}M)")
    
    # 3. å¯¹æ¯”ç»“æž„
    print("\n" + "-" * 40)
    print("ã€ç»“æž„å¯¹æ¯”ã€‘")
    print("-" * 40)
    
    # å¯¹æ¯” SAM
    print("\nðŸ” SAM ViT-B ç»“æž„å¯¹æ¯”:")
    sam_match = compare_state_dict_keys(original_sam, tower.sam_model, "SAM")
    
    # å¯¹æ¯” CLIP
    print("\nðŸ” CLIP-L ç»“æž„å¯¹æ¯”:")
    clip_match = compare_state_dict_keys(original_clip, tower.vision_model, "CLIP-L")
    
    # 4. è¯¦ç»†ç»“æž„æ‰“å°
    print("\n" + "-" * 40)
    print("ã€è¯¦ç»†æ¨¡åž‹ç»“æž„ã€‘")
    print("-" * 40)
    
    print("\nðŸ“‹ åŽŸå§‹ SAM æ¨¡åž‹å±‚:")
    print_model_layers(original_sam, prefix="  ", max_depth=2)
    
    print("\nðŸ“‹ åŽŸå§‹ CLIP-L æ¨¡åž‹å±‚:")
    print_model_layers(original_clip, prefix="  ", max_depth=2)
    
    # 5. æ€»ç»“
    print("\n" + "=" * 80)
    print("ã€æ€»ç»“ã€‘")
    print("=" * 80)
    
    if sam_match and clip_match:
        print("âœ… æ¨¡åž‹ç»“æž„å®Œå…¨ä¸€è‡´!")
    else:
        print("âš ï¸ æ¨¡åž‹ç»“æž„å­˜åœ¨å·®å¼‚ï¼Œè¯·æ£€æŸ¥ä¸Šæ–¹è¯¦æƒ…")
    
    print(f"\nå‚æ•°é‡å¯¹æ¯”:")
    print(f"  åŽŸå§‹ SAM:  {sam_params:,} vs é›†æˆ SAM:  {tower_sam_params:,} â†’ {'âœ… ä¸€è‡´' if sam_params == tower_sam_params else 'âŒ ä¸ä¸€è‡´'}")
    print(f"  åŽŸå§‹ CLIP: {clip_params:,} vs é›†æˆ CLIP: {tower_clip_params:,} â†’ {'âœ… ä¸€è‡´' if clip_params == tower_clip_params else 'âŒ ä¸ä¸€è‡´'}")
    
    return sam_match and clip_match


def compare_state_dict_keys(model1, model2, name):
    """å¯¹æ¯”ä¸¤ä¸ªæ¨¡åž‹çš„state_dict keys"""
    keys1 = set(model1.state_dict().keys())
    keys2 = set(model2.state_dict().keys())
    
    only_in_1 = keys1 - keys2
    only_in_2 = keys2 - keys1
    common = keys1 & keys2
    
    print(f"   å…±åŒçš„ keys: {len(common)}")
    print(f"   åªåœ¨åŽŸå§‹æ¨¡åž‹ä¸­: {len(only_in_1)}")
    print(f"   åªåœ¨é›†æˆæ¨¡åž‹ä¸­: {len(only_in_2)}")
    
    if only_in_1:
        print(f"   âš ï¸ åŽŸå§‹æ¨¡åž‹ç‹¬æœ‰: {list(only_in_1)[:5]}{'...' if len(only_in_1) > 5 else ''}")
    if only_in_2:
        print(f"   âš ï¸ é›†æˆæ¨¡åž‹ç‹¬æœ‰: {list(only_in_2)[:5]}{'...' if len(only_in_2) > 5 else ''}")
    
    # å¯¹æ¯”shape
    shape_mismatch = []
    for key in common:
        shape1 = model1.state_dict()[key].shape
        shape2 = model2.state_dict()[key].shape
        if shape1 != shape2:
            shape_mismatch.append((key, shape1, shape2))
    
    if shape_mismatch:
        print(f"   âš ï¸ Shape ä¸åŒ¹é…:")
        for key, s1, s2 in shape_mismatch[:5]:
            print(f"      {key}: {s1} vs {s2}")
    else:
        print(f"   âœ… æ‰€æœ‰å…±åŒ keys çš„ shape ä¸€è‡´")
    
    return len(only_in_1) == 0 and len(only_in_2) == 0 and len(shape_mismatch) == 0


def print_model_layers(model, prefix="", max_depth=2, current_depth=0):
    """æ‰“å°æ¨¡åž‹å±‚ç»“æž„"""
    if current_depth >= max_depth:
        return
    
    for name, module in model.named_children():
        num_params = sum(p.numel() for p in module.parameters())
        print(f"{prefix}â”œâ”€ {name}: {type(module).__name__} ({num_params:,} params)")
        if current_depth < max_depth - 1:
            print_model_layers(module, prefix + "â”‚  ", max_depth, current_depth + 1)


def test_deepencoder_tower():
    """æµ‹è¯•DeepEncoderVisionTowerçš„åŸºæœ¬åŠŸèƒ½"""
    print("=" * 60)
    print("Testing DeepEncoder Vision Tower")
    print("=" * 60)
    
    # ç›´æŽ¥å¯¼å…¥ä»¥é¿å…å¤æ‚ä¾èµ–é“¾
    import importlib.util
    spec = importlib.util.spec_from_file_location(
        "deepencoder_tower", 
        os.path.join(os.path.dirname(__file__), "llava/model/multimodal_encoder/deepencoder_tower.py")
    )
    deepencoder_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(deepencoder_module)
    DeepEncoderVisionTower = deepencoder_module.DeepEncoderVisionTower
    
    # åˆ›å»ºmock args
    class Args:
        mm_vision_select_layer = -2
        mm_vision_select_feature = 'patch'
        unfreeze_mm_vision_tower = False
    
    args = Args()
    
    # æµ‹è¯•ä¸åŠ è½½é¢„è®­ç»ƒæƒé‡çš„æƒ…å†µ
    print("\n1. Testing with random initialization (no pretrained weights)...")
    tower = DeepEncoderVisionTower(
        vision_tower=None, 
        args=args, 
        delay_load=True
    )
    tower.load_model()
    
    print(f"   - Hidden size: {tower.hidden_size}")
    print(f"   - Num patches: {tower.num_patches}")
    print(f"   - Num patches per side: {tower.num_patches_per_side}")
    print(f"   - SAM image size: {tower.sam_image_size}")
    print(f"   - CLIP image size: {tower.clip_image_size}")
    
    # æµ‹è¯•forward pass
    print("\n2. Testing forward pass...")
    dummy_input = torch.randn(2, 3, 336, 336)  # æ ‡å‡†è¾“å…¥å°ºå¯¸
    
    with torch.no_grad():
        output = tower(dummy_input)
    
    print(f"   - Input shape: {dummy_input.shape}")
    print(f"   - Output shape: {output.shape}")
    print(f"   - Expected output: [batch=2, num_patches, hidden_size=2048]")
    
    assert output.shape[-1] == 2048, f"Expected hidden size 2048, got {output.shape[-1]}"
    print("   âœ“ Hidden size correct!")
    
    print("\n" + "=" * 60)
    print("DeepEncoder Tower Test PASSED!")
    print("=" * 60)
    
    return tower


def test_with_pretrained_weights(model_path):
    """æµ‹è¯•åŠ è½½DeepSeek-OCRé¢„è®­ç»ƒæƒé‡"""
    print("\n" + "=" * 60)
    print(f"Testing with pretrained weights from: {model_path}")
    print("=" * 60)
    
    # ç›´æŽ¥å¯¼å…¥ä»¥é¿å…å¤æ‚ä¾èµ–é“¾
    import importlib.util
    spec = importlib.util.spec_from_file_location(
        "deepencoder_tower", 
        os.path.join(os.path.dirname(__file__), "llava/model/multimodal_encoder/deepencoder_tower.py")
    )
    deepencoder_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(deepencoder_module)
    DeepEncoderVisionTower = deepencoder_module.DeepEncoderVisionTower
    
    class Args:
        mm_vision_select_layer = -2
        mm_vision_select_feature = 'patch'
        unfreeze_mm_vision_tower = False
    
    args = Args()
    
    tower = DeepEncoderVisionTower(
        vision_tower=model_path,
        args=args,
        delay_load=False
    )
    
    # æµ‹è¯•forward
    dummy_input = torch.randn(1, 3, 336, 336)
    with torch.no_grad():
        output = tower(dummy_input)
    
    print(f"\n   - Output shape: {output.shape}")
    print(f"   - Output mean: {output.mean().item():.6f}")
    print(f"   - Output std: {output.std().item():.6f}")
    
    print("\n" + "=" * 60)
    print("Pretrained Weights Test PASSED!")
    print("=" * 60)
    
    return tower


def test_build_vision_tower():
    """æµ‹è¯•é€šè¿‡builderæž„å»ºDeepEncoderVisionTower (æ¨¡æ‹Ÿbuilderé€»è¾‘)"""
    print("\n" + "=" * 60)
    print("Testing build_vision_tower logic with use_deepencoder=True")
    print("=" * 60)
    
    # ç›´æŽ¥å¯¼å…¥ä»¥é¿å…å¤æ‚ä¾èµ–é“¾
    import importlib.util
    spec = importlib.util.spec_from_file_location(
        "deepencoder_tower", 
        os.path.join(os.path.dirname(__file__), "llava/model/multimodal_encoder/deepencoder_tower.py")
    )
    deepencoder_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(deepencoder_module)
    DeepEncoderVisionTower = deepencoder_module.DeepEncoderVisionTower
    
    class MockConfig:
        mm_vision_tower = None
        vision_tower = None
        use_deepencoder = True
        deepencoder_mode = 'base'
        deepencoder_base_size = None
        deepencoder_image_size = None
        deepencoder_crop_mode = None
        mm_vision_select_layer = -2
        mm_vision_select_feature = 'patch'
        unfreeze_mm_vision_tower = False
        s2 = False
    
    config = MockConfig()
    
    # æ¨¡æ‹Ÿ builder.py çš„é€»è¾‘
    deepencoder_mode = getattr(config, 'deepencoder_mode', 'base')
    deepencoder_base_size = getattr(config, 'deepencoder_base_size', None)
    deepencoder_image_size = getattr(config, 'deepencoder_image_size', None)
    deepencoder_crop_mode = getattr(config, 'deepencoder_crop_mode', None)
    
    tower = DeepEncoderVisionTower(
        config.vision_tower, 
        args=config,
        mode=deepencoder_mode,
        base_size=deepencoder_base_size,
        image_size=deepencoder_image_size,
        crop_mode=deepencoder_crop_mode,
        delay_load=True
    )
    tower.load_model()
    
    print(f"   - Tower type: {type(tower).__name__}")
    print(f"   - Hidden size: {tower.hidden_size}")
    print(f"   - Mode: {tower.mode}")
    print(f"   - SAM image size (base_size): {tower.sam_image_size}")
    print(f"   - Local image size: {tower.local_image_size}")
    print(f"   - Crop mode: {tower.crop_mode}")
    
    assert tower.hidden_size == 2048, "Expected hidden size 2048 for DeepEncoder"
    assert tower.mode == 'base', "Expected mode 'base'"
    assert tower.sam_image_size == 1024, "Expected base_size 1024 for base mode"
    print("   âœ“ Build test passed!")
    
    print("\n" + "=" * 60)
    print("Builder Test PASSED!")
    print("=" * 60)


def test_deepencoder_modes():
    """æµ‹è¯•ä¸åŒçš„DeepEncoderæ¨¡å¼"""
    print("\n" + "=" * 60)
    print("Testing DeepEncoder Modes")
    print("=" * 60)
    
    # ç›´æŽ¥å¯¼å…¥ä»¥é¿å…ä¾èµ–é—®é¢˜
    import importlib.util
    spec = importlib.util.spec_from_file_location(
        "deepencoder_tower", 
        os.path.join(os.path.dirname(__file__), "llava/model/multimodal_encoder/deepencoder_tower.py")
    )
    deepencoder_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(deepencoder_module)
    DeepEncoderVisionTower = deepencoder_module.DeepEncoderVisionTower
    
    class Args:
        mm_vision_select_layer = -2
        mm_vision_select_feature = 'patch'
        unfreeze_mm_vision_tower = False
    
    args = Args()
    
    # æµ‹è¯•æ‰€æœ‰é¢„å®šä¹‰æ¨¡å¼
    modes = {
        "tiny":   {"base_size": 512,  "image_size": 512,  "crop_mode": False},
        "small":  {"base_size": 640,  "image_size": 640,  "crop_mode": False},
        "base":   {"base_size": 1024, "image_size": 1024, "crop_mode": False},
        "large":  {"base_size": 1280, "image_size": 1280, "crop_mode": False},
        "gundam": {"base_size": 1024, "image_size": 640,  "crop_mode": True},
    }
    
    print("\nðŸ“‹ æµ‹è¯•é¢„å®šä¹‰æ¨¡å¼:")
    for mode_name, expected in modes.items():
        tower = DeepEncoderVisionTower(
            vision_tower=None, 
            args=args, 
            delay_load=True,
            mode=mode_name
        )
        
        assert tower.sam_image_size == expected["base_size"], \
            f"Mode {mode_name}: Expected base_size {expected['base_size']}, got {tower.sam_image_size}"
        assert tower.local_image_size == expected["image_size"], \
            f"Mode {mode_name}: Expected image_size {expected['image_size']}, got {tower.local_image_size}"
        assert tower.crop_mode == expected["crop_mode"], \
            f"Mode {mode_name}: Expected crop_mode {expected['crop_mode']}, got {tower.crop_mode}"
        
        print(f"   âœ… {mode_name}: base_size={tower.sam_image_size}, image_size={tower.local_image_size}, crop_mode={tower.crop_mode}")
    
    # æµ‹è¯•è‡ªå®šä¹‰è¦†ç›–
    print("\nðŸ“‹ æµ‹è¯•è‡ªå®šä¹‰å‚æ•°è¦†ç›–:")
    tower = DeepEncoderVisionTower(
        vision_tower=None,
        args=args,
        delay_load=True,
        mode="base",
        base_size=800,
        image_size=400,
        crop_mode=True
    )
    
    assert tower.sam_image_size == 800, f"Expected custom base_size 800, got {tower.sam_image_size}"
    assert tower.local_image_size == 400, f"Expected custom image_size 400, got {tower.local_image_size}"
    assert tower.crop_mode == True, f"Expected custom crop_mode True, got {tower.crop_mode}"
    print(f"   âœ… è‡ªå®šä¹‰: base_size={tower.sam_image_size}, image_size={tower.local_image_size}, crop_mode={tower.crop_mode}")
    
    print("\n" + "=" * 60)
    print("Mode Test PASSED!")
    print("=" * 60)


def main():
    parser = argparse.ArgumentParser(description="Test DeepEncoder integration")
    parser.add_argument(
        "--deepseek_ocr_path", 
        type=str, 
        default=None,
        help="Path to DeepSeek-OCR model weights (optional)"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cpu",
        help="Device to use (cpu/cuda)"
    )
    parser.add_argument(
        "--compare_structure",
        action="store_true",
        help="Compare model structure between original DeepEncoder and integrated version"
    )
    args = parser.parse_args()
    
    print("\n" + "#" * 60)
    print("# GEM + DeepEncoder Integration Test")
    print("#" * 60)
    
    # å¦‚æžœæŒ‡å®šäº† --compare_structureï¼Œåªè¿è¡Œç»“æž„å¯¹æ¯”
    if args.compare_structure:
        compare_model_structure()
        return
    
    # åŸºæœ¬åŠŸèƒ½æµ‹è¯•
    test_deepencoder_tower()
    
    # æ¨¡å¼æµ‹è¯•
    test_deepencoder_modes()
    
    # Builderæµ‹è¯•
    test_build_vision_tower()
    
    # é¢„è®­ç»ƒæƒé‡æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
    if args.deepseek_ocr_path and os.path.exists(args.deepseek_ocr_path):
        test_with_pretrained_weights(args.deepseek_ocr_path)
    else:
        print("\n[Info] Skipping pretrained weights test (no path provided)")
    
    print("\n" + "#" * 60)
    print("# ALL TESTS PASSED!")
    print("#" * 60 + "\n")


if __name__ == "__main__":
    main()

